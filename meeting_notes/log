22 November
Läser https://arxiv.org/pdf/2011.03545.pdf på jakt efter en resolution function. 
    Hittar någon slags efficiency-parameter eta_DOM. Kan denna användas?
    Fig B.1 visar right ascencion distribution med error bars. 

I https://arxiv.org/pdf/2005.02395.pdf Står det 
    "Due to their topology, track events have a better angular resolution of ∼ 0.5
◦   at the highest energies"
    Dom citerar https://icecube.wisc.edu/science/data/PS-3years som en källa till angular resolution. Datafilen ger mig ett angular error per event.

I https://hepconf.physics.ucla.edu/pacific11/talks/chirkin.pdf 
    står det på s.23 att cascades har en energyresolution på 10% på log_10(E)
    s.30: Resolution in log10(E):
            • muons: ~0.25
            • cascades: ~0.1-0.2 
    Resolution in log10(E):better with spline-fitted parametrization of ice

https://arxiv.org/pdf/1612.05093.pdf är en beskrivning av IC. Läs! s 15,16

https://arxiv.org/pdf/1311.4767.pdf
    s.7:  Charge resolution obtained by this
    method, which is dominated by the width of the charge response of the Hamamatsu R7081 photomultiplier [14, 21],
    is typically around 30% at the single photon level

    s.12: The deposited energy resolution for contained νe events
    is dominated by statistical fluctuations in the collected
    charge at low energies, improving from 30% at 100 GeV
    to 8% at 100 TeV, at which point the extension of the
    shower begins to distort the reconstruction (see Fig. 13).
    This high-energy limit on resolution is similar to uncertainties in the modeling of scattering and absorption in the ice
    sheet [17, 18], which contribute a 10% systematic uncertainty to the energy.

    Fig13,14,21 verkar visa det vi vill ha?

    s. 19:  IceCube achieves
    average deposited energy resolution in all channels of ∼
    15% (depending on the event selection used in individual
    analyses, better resolutions are possible). This is limited
    primarily by systematic uncertainties above a few TeV

https://indico.cern.ch/event/221841/contributions/1525814/attachments/362223/504242/Kopper_-_TeVPA_HESE_technical.pdf
    muon track: <1 deg angular resolution, factor of 2 energy resolution
    nc/e cascade: 10 deg angular, 15% energy
    angular resolution depends on the ice model

24 November
single_41 och att diagonalisera 3x3-blocket verkar vara ekvivalenta. Börjar lämna dentonoch kör på egenvektorer istället. 
är s34 != 0 får vi problem, eftersom Um1 = s34²/(t24²+s34²) och Ut1 = t24²/(t24²+s34²). Dessa är alltså helt oberoende av både energi och är då konstanta.
När s4 =0.04 blir dessa konstant 0.48 resp 0.51 istället för 0 resp 0 som förutsägs av Us. Mycket märkligt. Betyder detta att single_41 inte fungerar? 
I Us så beror dessa element på s14. Enda möjligheten just nu är då att thetaM14 blir !=0, trots att theta14=0...

25 November
10.1016/j.astropartphys.2012.01.004 har grafer på A_eff vs E

10.1103/PhysRevLett.117.071801
    "The muon zenith angle
    can be reconstructed geometrically with a resolution of
    σ cos(θz ) varying between 0.005 and 0.015 depending on
    the angle."

    "Muon energy is reconstructed based on the
    stochastic light emission profile along the track [16, 34]
    with a resolution of σ log 10 (E μ /GeV ) ∼ 0.5."

Laddar ner lite IC data-filer och kollar igenom
I en C-fil läser vi
    //To compute the expected number of observed events we need to:
	//1a. Multiply each effective area by the average flux in each bin
	//1b. Multiply by the phase space in each bin (true energy and solid angle)
	//1c. Multiply by the livetime for which the effective area is relevant
	//2. Sum over the effective areas for all particle types and detector configurations
	//3. Sum over dimensions not of interest (true neutrino energy, possibly zenith angle)
	//In this case we will compute the expectation as a function of the energy
	//proxy only, so we will project out both true energy and zenith angle.

    //the product of the average flux in the bin and the
	//phase space in the bin is simply the integral of the
	//flux over the bin
Finns en jupyter notebook med intressanta kommentarer, bland annat ett fint oscillogram jag kan använda till uppsatsen. Datat är dock simulerat och inte användbart.
Labels skrivs som plt.xlabel(r"$L_{\text{eff}}$ (erg yr${}^{-1}$)")
För att få modifierad baseline för nedgående flux:
    # Get baseline [km] (propagation distance) from zenith angle
    # Unit is km
    true_coszen = np.cos(true_zenith)
    earth_radius = 6371.
    production_height = 15. # Assuming neutrino produced 15 km above surface
    detector_depth = 1. # Assuming detector depth of 1 km
    baseline = -earth_radius*true_coszen +  np.sqrt( (earth_radius*true_coszen)**2 - earth_radius**2 + (earth_radius+production_height+detector_depth)**2 )
Finns också som eq 11.49 i giunti
Jag har:
    -Tabulated total effective areas as a function of neutrino energy and declination for the 2012-2015 years.
    -Measurements of the median angular resolution for simulated through-going muon neutrinos and antineutrinos events as a function of neutrino energy. For 2012-2015
    -Aeff från IC86 2012 som funktion av energi och zenith
    -Events från IC86 2012 med individuella angular errors
    -Cfil med en readme som pratar om effective areas och använder dessa.

Borde fundera på hur binning görs. Ifall båda sidor är inklusive får jag ju dubletter. Finns litteratur på detta?

27 November
Info om X för TeV finns här https://www.nature.com/articles/nature24459

30 November
Finns jättenice mathematica+python kod i IC86SterileDataRelease för både plottar och metodik
Vill jag använda en z-oberoende angErr från 2015 eller en z-beroende angErr från 2012?


1 December
Använder z-beroende data från 2012 för angErr. CT interpolation fungerar bra för 2d interpolation. 
Just nu beräknar jag en interpolator för z_min och en för z_max. Tar sedan medelvärdet av det interpolerade värderna som resultat. Kolla detta med S.
Jag börjar kolla på Gaussian smearing och om man kan integrera någon Gaussian.
IC2014 har en graf på deposited energy vs reconstructed energy. Där kan jag få sigma_E som funktion av E^r. Dock frpn 2014.
IC2016 ger sigma_z mellan 0.005 och 0.015. Samma artikel ger sigma_logE som 0.5
IC2017 (10.1103/PhysRevD.95.112002) ger "median energy resolution" vid 8 GeV som 30%, och 20% vid 20 GeV. Datafilen (som ligger på mitt skrivbord) Ger Aeff för olika flavors och CC/NC. Den är dock binnad i både E och z.
Ska kolla IC2020 och deras companion paper också. Samt läsa S artikel som tog med sånt här ganksa detaljerat.

2 December
Supplementary IC2020 (https://link.aps.org/doi/10.1103/PhysRevD.102.052009):
    "The observed zenith
    angle can be taken as the true zenith angle, θ_reco =  θ_z , for
    practical purposes, since within our angular bins the
    difference in zenith angle between the reconstructed muon
    track and the MC truth is negligible"
Innehåller dock inget mer jag akn använda just nu.
https://docushare.icecube.wisc.edu/dsweb/Get/Document-73829/weaver_thesis_2015.pdf (som refereras av IC2020) innehåller en graf över Aeff och z.
Den visar att Aeff i princip är ganska oberoende av z mellan 1e2 och 1e4 GeV! Speciellt kring 2 TeV verkar z-beroendet vara mycket litet.
Denna thesis verkar ganska bra, metodisk. Jag kanske borde kolla på med phd theses för en mer low-level genomgång.

interpolera med bisplrep?
If i use other interpolators,i might get a better integration
https://stackoverflow.com/questions/44811581/scipy-how-to-integrate-a-linearly-interpolated-function

4 December
Borde göra 4 interpolatorer för Aeff istället för två. Finns ingen anledning att tro att vi har en "max" med Emax och zmax och en "min".
AngErr ger vad som ser ut som en ganska rät linje. Förbättra denna lite längre fram.

8 December
Implementerat symbolisk diagonalisering i en notebook. Min single approx verkar ge precis samma beteende för TeVoch uppåt. Vi kan alltså
dela in beteendet av U i två delar: en aktiv del för energier upp till 100 GeV, och en steril del för energier över 100 GeV. Min approximation
ger konstanta matriselement i den aktiva delen, men nästan exakt med den fulla Hamiltonianen överensstämmande element i den sterila delen.
Sympy verkar trögt med att symboliskt diagonalisera H, men får ha tålamod. Ska dubbelkolla hur formen av mitt U ser ut och jämföra detta med den symobliska.
Ska be S om hjälp angående resolution function som inte blir helt rätt. Något med en lognormal fördelning som jag inte riktigt förtår.

9 December
S verkade inte orolig över problemet med Gaussianen, jag kör på en vanlig normalfördelning så länge Jag har extraherat sigmorna från IC2014 och interpolerat dessa.
Plottar jag en konturplot så ser det lite konstigt ut, jag förväntar mig att det ska reproducera Fig 13a: ett linjärt samband mellan Etrue och Ereco, men en liten gaussiansk spridning.
Nu verkar det bara gälla vid låga energier, men dom vanliga 2d-plottarna ser bra ut, så det är säkert bara något som min gauss() inte förstår med np.meshgrid. Detta problem borde inte 
vara relavant för själva integrationen, så jag hoppar nog det tills vidare.

12 Jan 
Läser hela IC2020 companion paper. Behöver implementera starthöjd i numeriska P-koden. Pappret säger att fluxet startar vid en höjd på 20 km.

13 Jan
Jag kan reproducera oscillogrammen för anum disapperance i IC2020. En skillnad är ju att min kod inte har implementerad neutrino absorption, vilket ger en alldeles för låg disappearance probability för höga (1e5 GeV) energier med låg (-1) zenith.
För att kontrollera multiplicering och integrering av meshgrids: sätt alla arrays till OLIKA dimensioner. Har fixat detta för 3d, ska nog göra för 2d också

14 Jan
Använder romb instäälet för simps, och tagit bort onödigt funktioner. Fixat interpoleringen så alla z-värden verkar fungera bra.
Med rombs blir eventsen ganska bra! Verkar dock behöva integrera över ganska långa (40-50 steg) vektorer för att få konvergerade svar...

15 Jan
S säger att jag ska normera varje bin för nullhypotesen så jag får samma event per bin som null, och sen se hur 3+1 blir efter denna normering. Ett krav är dock att normeringen blir någorlunda konstant, vilket den inte blir nu. Detta betyder att det fortfarande finns jobb kvar att göra i event-koden. För null är ju denna oberoende av P-koden. jag ser att normeringskonstanterna för höga z är 1e2, medan den är ental för lägre. Detta kan betyda att jag behöver undersöka min interpolering för höga z. Behöverockså fixa extrapolering för flux, så jag kan låta Etrue något högre. Behöver även studera gaussianen lite mer för att se hur lång Etrue behöver vara.

18 Jan
Har kollat på gaussianen för första E-binnen mellan 500 och 630. Den behöver bara ha Etrue mellan 300 till 900. Kanske kan göra detta för alla E-bins och bygga en speficik Etrue-range för varje bin? Isåfall slipper jag ju integrera ut till 1e4 för låga energier.
Med denna metod får jag just nu första E_bin, förstaz-bin (dvs 500<E<630, -1<z<-0.95) till ca 5500, och första E_bin, sista z_bin (dvs 500<E<630, -0.05<z<0) till 13000. Jämför detta med korrekt 2336,2021. Vi har alltså en faktor 2 och 6 off. Denna faktor är z-beroende. Inget fel i gaussian eller Aeff pga inget z-beroende. Dock är flux_m ca 0.4 för z=-1 och 1 för z=0. Denna måste vara boven. Hur kan IC ha 13% färre events för höga z när flux ökar med upp till 150%? Jag har jämfört H0 med events, men skillnaden borde inte vara så stor. Denna höga ökning ser vi dock i energier kring 1e3, när events går från 911 till 4.2k
    1. Fundera på variabelsubst. z -> cos theta
    2. Leta efter artiklar där man gjort detta. Vet att S visade något någonggång som jag borde sparat.l
    2. Kontrollera flux data.
    3. Kanske ska testa med z-beroende Aeff? Har ju Aeff(decl)...

Jag har bara sigma_E för num mellan 1e4 och 1e7. Detta borde inte påverka problemet ovan, men kan förklara att mina events inte är lika utsmetade. Använder jag sigma_E = 0.5 i log-space så får jag en väldigt stor utsmetning (måste ha Et mellan 10 och 1e4 för första E_bin). Detta medför dock att jag behöver extrapolera flux till 1e5. Jag får även en konstig normering. Dock är kvoten mellan första zenith bin och sista ca 0.5, och den var 0.43 innan. Ingen direkt skillnad... Borde inte en sigma = 0.5 i logspace vara likvärdigt med linjär sigma =0.1? Får dock helt annan gaussian. När jag använder konstant sigma=0.4/0.5 och använder log_E i gaussianen får jag ganska rimliga events. 

När jag plottar en imshow av flux ser jag rätt form, med maximum i låga E och höga z. Denna peak spetas ut av både Aeff (som ökar för högre energier) och gaussianen. Detta känns bra. Jag förstår dock inte första raden i ICs events. Baserat på flux borde den inte vara så låg...

19 Jan
Har extrapolerat flux till 1e5 och skrivit en funktion som returnerar Etrue range for Ereco bins. 
Får nu fram en normaliseringsmatris med värden mellan 0.1 och 1.2 ca. Rätt bra, men mina events är fortfarande för oberoende av z. Ska kolla hur det blir med punkterna för 18 jan. Mina bästa värden är dock kring 1e3 GeV, vilket är bra! Funderar fortfarande på lognormal... Använder den nu, vilket verkar fungera. Dock osäker på hur korrekt integreringen blir. Kanske ska skriva om alla interpolatorer i logE.

20 Jan
Har mixtrat mycket med Aeff. Har fått in zenith dependence genom deklination nu. Har två olika metoder, där den ena försöker använda ett viktning mellan de två närliggande interpolatorerna. Det ser okej ut förutom vid gränserna mellan två interpolatorer pga logaritmisk data. Istället kan man bara välja den närmsta interpolatorn, och köra på den. Ett tredje alternativ är att jag kör på med CT (som vid flux), och fixar den extrapoleringen som behövs vid låga z. Ifall jag använder CT med medelvärdesdata behöver jag fixa interpolatorn för z< -0.86 ca.

21 Jan
Fixar analytisk jupyter-kod.
dm41-approximationen verkar leda till att Ux4 är 0 eller 1 för alla energier, givet att theta_14=0. Detta leder dock inte till att alla sannolikheterär noll (vilket jag först trodde) eftersom dmM är nollskiljd, även om dm är 0 i vakuum!
Får nu den analytiska lösningen med dmM och UM att stämma med den numeriska! Den är givetvis inte supernära, och min single-approximation är ännu lite mer off, men detta borde räcka för att kunna argumentera för att single-approx är användbar!

22 Jan
Har lagt in mbar-events, som dock använder Aeff för muoner. Kanske borde jämföra cross sections för mu och antimu för att se hur stor skillnaden blir.
Har fixat till Aeff_2012 i deprecated.py ifall ovan försök med aeff_2015 inte fungerar. Just nu verkar den ge allt för låga events dock.
doi:10.3847/1538-4357/835/2/151 eq 1 är en referens till vår event integral

Använder jag begränsad Et med get_Etrue_limits får jag en ganska annorlunda event-fördelning.Vet ej hur mycket detta beror på den finare indelningen,och hur mycket som beror på kortare integratiosgränser. Denna ger en faktor mellan 4 och 5 i intervallet 997-1581 GeV jämfört med 1-2 för konstanta Etrue-gränser. Har nu jämfört, och get_Etrue_limits ger mycket bättre events.
Jag jämför Aeff_2012 med Aeff_2015, och ser att dom är off med en faktor 20... Aeff_2012 verkar ge bättre resultat och har zenith dependence, så jag väljer det istället för 2015 datat.

Energy resolution är inte linjär. Jag kanske vill fixa detta? B.la. Weavers thesis skriver om detta (10 TeV Etrue ger 5 TeV Ereco). Vet dock inte hur mycket detta påverkar pga binning.
Har nu fixat detta. Vill göra två interpolatorer som kan relatera reco <-> true, och sen rekonstruera bilden i weaver(IC2014)

26 Jan
Bestämt mig för aeff 2012. 2015-datan ger konstiga areor med en faktor 1e3, det var därför mina events blev någorlunda nära. Nu är dom 1e3 off, men jag har å andra sidan en bra aeff. Vet inte var denna faktor kommer ifrån, men normaliseringen blir bra.
En snabb analys av inkluderingen av astrophysical och prompt flux ökar mina events med någon procent. Bra, men inte tillräckligt. Implementera den icke-linjära gaussianen!

27 Jan
Tror j
ag fått till den icke-linjära gaussianen, dock stämmer inte riktigt standardavvikelsen överens med weavers sannolikhetshätheter. Använder jag sigma=0.3 blir den lite för bred, vilket är konstigt. Dock tror jag att jag kan ha fått till det viktigaste, dvs förskjutningen av Ereco till lägre energier. Kanske borde kolla detta närmare.
Har extrapolerat Aeff till sidorna för z. Ska göra samma med flux, där jag kan utnyttja att jag hr periodisk data. Läs in flux för z=0.05 och interpolera.

28 Jan
Fixat fluxes och städat lite kod. Skrivit en 2d event som stämmer överens med 2d events med pytteliten std. Fixade även att jag hade fel normering/integrationsgränser, vilket löste probolemet med att mina events var 1e3 off, eftersom jag logaritmerat faktorn utan att ha logaritmerade gränser. Jag tror att jag vill skriva en ny get_Etrue_limits för den nya resolution function. Funktionen ska returnera gränserna för någon percentil av gaussianen. Detta är hur jag ska inkorporera relationen mellan Ereco och Etrue. Min arbetshypotes är att jag inte kan ha för stora Etrue-gränser eftersom min mesh då blir för sparse.

29 Jan
Just nu får jag bäst events med den gamla felaktiga metoden med sigma=0.3 och integrate(integrand,'romb', np.log10(Et),zr,Er) / scale_factor. Eftersom gaussianen integreras till 1 med exakt dessa integrationsgränser och skalfaktor så drar jag slutsatsen att denna integrering är korrekt.
 
Nu använder jag mig av 99%-e percentilens gränser av energy resolution function för att få gränser för Etrue för ett givet Ereco. Denna metod känns rimlig. Får faktiskt ganska fina kurvor i E-led för TeV. Dock har z kurvan blivit lite sämre. Kan dock experimentera med sigma som just nu är väldigt låg på 0.2. Funderar på om jag inte borde maila lite IC-människor och beskriva min algoritm för S. Ett till alternativ vore att begränsa eventsen jag kollar på i z-led. Just nu får jag dålig kurva för höga z. Kanske ska föreslå detta för S. 

01 Feb
När sigma är >= 0.2 har vi inte stort nog Aeff. Alltså måste vi:
    1. Nöja oss med sigma < 0.2
    2. Ta fram Aeff för Ereco = 1e2
    3. Begränsa E bins

03 Feb
Testat att digitiza höger plot från weaver och fittat den till några gaussianer. Det gick bra, men sigma är ca 0.5, vilket ger alldeles för stora gränser på Etrue. Hoppas på att antigen få tag på mer Aeff-data för E<1e2 GeV, eller på att min nuvarande sigma=0.2 fungerar okej. Hittat ett repo som jag har clonat och ska kolla på mer imorn.

04 Feb
Från github hittade jag hur man använder lognormal-fördelningen med procentvärden. Detta verkar inte ge så stor skillnad, dessutom måste jag manuellt skriva in Etrue-gränser som tidigare. Kanske kan kika mer på det senare, men är ju z som strular nu.


08 Feb
Att normera antalet händelser i ena ledet verkar både ge sämre MAE som chi2, jämfört med min metod som minimerar MAE för en samma konstant i båda led. Slutsats: Eventfelet i de olika leden har för olika form för att kunna normera ensidigt. Det är därför bäst att normera varje hink enskilt. Dock så skadar det inte att jag testar denna metod med 3+1/3+2 hypotesen. 

12 Feb
Fel i ic_params? Vad är theta_22?
IC har ingen resonans pga andra parametrar.
Generera P för dm_41=-1?

17 Feb
MC som jag kan få tag på är tyvärr bara mellan 1e0 till 1e2 GeV. Det enda jag kan göra med dessa är att hitta energy resolution och Aeff vid dessa energier. Lite orolig över mitt val av energy resolution...

23 Feb
Energy resolution ser bra ut, kanske kan mixtra lite med cv och hyperparametrar samt större sample, men det är lite slöseri med tid. Framtagandet av Etrue är korrekt, jag vill inte sampla Etrue från lognormal, eftersom multiplicerandet av pdfen tar hand om den fördelningen.
Inte säker på att denna aeff är bättre. För N=10 verkar den nästan sämre. Men ska kolla med högre N och fler parametrar. Annars byter jag bara tillbaka.
Mina events viker ner vid låa energier lite för tidigt nu. Men kan bero på parameter + N. Har j dm41=0.1 vilket lägger resonansen tidigare än vad jag är van vid

24 Feb
Det är fel i MC-datat. Artikeln visar ingen rate-dip vid 1e3, och jag har en markant dip där i flera bins. Detta är ju givetvisoberoende av flux,Etrue osv. Detta förklarar den stora skillnaden mellan obs och MC. 
Gaussianen verkar aldrig integrera till under 1. Vill testa att peta på left_alpha. 0.95 verkar som ett bra värde. 
Kanske vill skriva om lite i event_processing så allt är cleant i bin_by_bin. 
Litar inte fullt ut på varken min flux-extrapolering,eller på den nya aeff... Kanske ska dubbelkolla fluxet och testa med gamla aef från 2012 ifall ovan inte funkar.
Kör jag MC-filen med mitt flux får jag 4% fler events. Det är alltså inte så stor skillnad på fluxen som jag trodde
Det är inte fel i MC-datat. Grafen har Ereco på x-axeln och jag har Etrue.
MC stämmer dåligt överens med obs. Kan jag ens använda MC då?

25 Feb
I giuntis global 3+1 analysis arXiv:1703.00860v3 står det "we tested that our results do not change significantly if another model is used instead of
the Honda-Gaisser one." Denna artikel innehåller en hyfsad genomgång om IC metodiken. 
Kolla på ai platform för GPR

28 Feb
Med den "nya" aeff för jag chi2 till 10.8k (onormerad), jmf med 7k för aeff_2012. MC aeff strular så mycket och är sämre...
Nya aeff ger bågformade z-events. Detta förklarar varför MC-eventsen är konvexa.
Lägger jag tillbaka skalfaktorn i z-led får jag återigen en bra fit med H0 för låga z. 
Dock ger detta dålig kontur...
Att restricta z-bins verkar ge lite bättre kontur.
Nu verkar filerna ha fuckat upp, generera nya med ny och gammal aeff

29 Feb
Giunti använder en annan chi2. Hur påverkar detta?
Testa mot 2017-data?
Just nu är 2012 Aeff bäst utan scale_factor.  Verkar dock inte spela så stor roll för konturerna! Skulle vilja begränsa och/eller rotera mina z-events.
2012 Aeff fittar extremt bra mot IC_MC_2017 för z< -0.4. Detta är konstigt. Borde betyda att extraheringen av 2017 aeff från MC_2017 är felaktig. 
2017 data, 2017 mc, och min kod (med 2012 Aeff) passar väldigt bra. Min kod är ganska mycket för platt för z > -0.2
Dock ger 2020 data, 2017 Aeff, 2020 MC den bästa konturen för z < -0.5 !

Slutsats: För H0 passar 2017 data, 2017 MC, 2012 Aeff bäst. För kontur passar 2020 data, 2020 MC, 2017 Aeff bäst. Det 


03 Mar 
Icecube sätter sigma_a till 0.4 och delta_gamma till 0.03. Delta_gamma har ett best fit på 0.068
Salvado får best fit på delta_gamma som 0.02. Dom använder sigma_gamma=0.1
Esmaili avänder inte spectral alls. 

Lägg E_pivot i medianen?
För att få deltaN/N =sigma = 0.1 behöver jag delta_gamma=0.15. Högt!
Köra chi2 på detta?Låt sigma och delta_gamma vara helt fria för H0. 

05 Mar
N=9 ger för dålig contour fit. Gamma ger just nu ingen skillnad.
Flux factors i alla led verkar ge lite bättre för höga dm, men mycket sämre för låga, så skippa det för nu.

09 Mar
Lägre alpha ger lägre chi2. Minimum verkar vara vid 0.8-0.75. Dock ändras knappt konturen, om något blir den sämre när alpha=0.75
Konturen ser bättre ut när jag sätter z mellan 0 och 11.

10 Mar 
N=9 ger faktiskt inte så mycket sämre kontur än vad jag trodde (cf N=21), snarare bättre! Pyttelite sämre vid höga dm, men bättre för låga. Borde kunna köra N=9 för s34.

12 Mar
Öka till N=13 så att Etrue inte missar resonansen. 
For mariginalization: Find best fit th_34 and plot te regular contour with dm and s24.
We see no clear th34 effect below 1 eV2. 
    1. Rerun with finer parameter step sizes
    2. Incluce tau decay. Calculate Pmt and use 17% of this flux. num->nut->(17%) num 
Otherwise, leave 3+1 and
    1. NSI. Look up armands and marfatias papers on NSI. See which epsilon that affects Pmm.
    2. 3+2. Easy theory, but have more parameters that we need to fix beforehand.
    3. Lorentz invariance breaking / non-unitarity

15 Mar
Med en two-flavor approximation för atmospheric oscillations beror Pmm på ett, emt, och emm. Alltså tre extra parametrar.
Hur påverkar nsis vår cross-section/effective area?
Den nya simuleringen med N=13, s34=0, och högre upplösning ger en dålig kontur. Vad kan ha ändrats?
Gamla simuleringar med N=9 och N=21 ger bra konturer. Alltså borde inte förändringen i N vara boven.
Har tidigare sett att högre upplösning kan ge lite annan kontur. Kanske ska hålla mig till 10x10.
Denna upplösning är inte symmetrisk. Jag kan ha gjort fel i någon reshape och inte märkt det eftersom tidigare simuleringar varit symmetriska. 
Det jag är mest rädd för är att event-koden ka ha ändrats under tiden. Jag har ju ändrat hur U tas fram och hur U och M kommer in i numeriska koden. Jag tyckte dock att jag kollade att denna optimering 
inte påverkade P...

Alternativ:
    1. Simulera för N=13 med 10x10 och kanse även för N=11 med 10x10. Ifall dessa fortfarande är dåliga ligger problemet i ny P-kod. -> rollback av P . Om dom är bra betyder det
        att jag måste vara försktig med min upplösning
    2. Ta fram lite P för N=9 och jämför ifall dessa ändrats.
Har nu kört events för 10x10 med N=13 istället. Det blir en bra kontur!! Ska nu kolla upp om det är någon reshape... Ifall det inte är det (typ om 10x20 också är bra), är jaglite osäker på vad jag borde göra...
Problemet är löst! Det var en reshape som mixade elementen helt fel.
Utan Pmt har vi ingen skillnad när s34=s24. Detta är logiskt eftersom P bara ändras för s34=2*s24 vid dessa låga dm (0.1-1 Ev2). Tre alternativ:
    1. Skita i theta34
    2. Ta med tau decay
    3. Ta med cascades 
Resonansen kommer bara i en-två hinkar. Även om eventsen blir något högre mha tau så är detta inte tillräckligt för att flytta konturen överallt eftersom effekten bara finns i en-två hinkar, och bara för högre (>0.5 dm41). Simulera N=13 20x30x30 där 2*s24=s34.
Efter detta kan jag prata med S och visa att theta34 inte ger så stort grafiskt utslag. Först efter den diskussionen vill jag marginalisera. Kan visa henne min 7x7x7-plot för N=9.
Gör inget mer med th34 tills dess. 

16 Mar
Mina plots stämmer bra överens vid z=-1, men både NSI och 3+1 är helt off för E<1TeV vid z=-0.8. 
Skillnader:
    1. Baseline. Vid z=-0.8 färdas ju neutrinon kortare. Jag har tidigare jämfört detta med smirnov, så konstigt om det inte skulle funka nu. Har doch ändrat theta -> zenith. Ändra tillbaka?
    2. Mean density. Har dubbelkollat PREM. Globes kanske tar härsyn till något annat?
    3. 
Har fixat radie-funktionen, men ingen skillnad. Dock så ger den korrekt core-mantle boundary vid z=-0.83, så den borde vara rätt.
Fixat! Problemet var att ekvationen integrerades i 12 km oavsett zenith. Nu borde alla P behöva räknas om :))

19 Mar
Plot the impact of tau decay (ignore IC contour match)
Then show impact of theta24=theta34 

See if I can find DC MC and events for NSI. Also, include all E bins into NSI sims. 

e_ss = 0 gives a NSI resonance. Simulate using global best fit of dm_41 (0.93 according to Kopp), and make a contour on theta_24 vs e_mm-e_tt. We wont have same problem as Marfatia since we don't try to 
remove the resonance with e_ss. Simply constraining.
See which e changes P, and take those e ranges. 
Then we can make two contour:
    th_34=0, e=0
    th_34=0. e!= 0

For 3+2: Look at Pmm for each theta and see which parameters are relevant. Those who are irrelevant can be put to zero, those who are somewhat relevant can be put equal to eachother.

Um4,Ut4,Us4 reduceras helt perfekt till vakuum-värdena. De andra elementen verkar inte göra det. De är dessutom prop 1/A. För theta_34=0 verkar alla element reducera till vacuum (för A=1e-10). För alla andra fall
fungerar detta inte. Alltså:
    Jag har perfekta Um4,Ut4,Us4.
    Alla andra element fungerar för theta_34=0
Får ej Pmt att funka

22 Mar
Har hittat DC-86 data och MC i E-led. Ska fråga S om jag ska inkludera atm muoner eller inte. Skulle alltså kunna göra DC analysen normerat i E-led.
Pmm Resonansen för th_34=0 är oberoende av e_tt. Arman säger att vi måste välja e_mm och e_tt nästan lista stora för att inte sabba DC fit. 
Pmm Resonansen för th_34=th_24 är svaaagt beroende av e_tt. Dalarna blir något djupare med lägre e_tt, men det är knappt.
Marfatia säger att non-zero e_mt är strongly constrained.
Med e_mm translaterar vi Pmm resonansen.

24 Mar
Får inte DC data att funka med min extrapolerade Aeff. Har dock hittat väldigt bra DC aeff från 2016. Känner just nu att jag vill släppa DC och "lita" på arman. Behöver jag verkligen simulera hela DC 
för att kunna säga att e_mm-e_tt ska vara liten? Jag har ju redan 90% CL från någon konferens. Kör om denna med finare värden och kortare range i e-led och längre range i th-led.
Kör en sim där du marginaliserar icke-NSI över dm.
Gällande 3+2 verkar th25 bidra på samma sätt som th24. En extra grej här är att th35 bidrar till två resonanser, så även fast effekten av th24 uteblev i 3+1 kanske den syns i 3+2.
Just nu känns 3+2 lite bökigt, men kan ju ha det som backup

25 Mar
Har gjort om strukturen helt, och märkt att den precomputade aeff antagligen var den dåliga från 2015. Har genereat den gamla igen. Ifall något fuckar så kan jag alltid reverta.
Jämför standardkonture med s34=0 mot Giuntis och kolla att din nya fortfarande är bättre. Oavsett så kanske det ändå är intressant att kolla hur th34-konturen påverkas?

29 Mar 
DC: För varje E och z-bin i reco har jag en aeff för varje uppsättning Etrue,ztrue. Antingen summerar jag över alla reco, och får då en snitt-aeff för Etrue,ztrue. Jag kan sen använda den osummerade dfen för att träna/ta fram E och z resolution som tidigare.
Eller så kör jag på "som dom vill", behöver då beräkna ca 200 probs för varje reco bin. Har då totalt 4 gånger fler probs än IC vid N=13. 
Tredje alternativet är att beskära dfen lite för att få ner antalet probs som behöver beräknas per bin. 

30 Mar
DC 2018 MC stämmer varken överens med DC 2015 MC eller min null...
Min null stämmer jättebra överens med DC 2015 i z-led, men dåligt i E-led. 
Just nu känns det bäst att använda DC 2015. Avvikelsen hos DC2018 kan bero på annan event selection och muon background.

31 Mar
Salvado 2017 constrainade -6e-3 < e_mt < 5.4e-3 med IC86 data från sterile datasettet. Dom har också en closed contour med e' vs e_mt. Dom tittade bara på Pmt, och 
    kunde därför inte constraina e', så dom satte en prior på den till |e'| < 0.049
Demidov 2020 använde DC data från 2019 (som jag kanske vill andvända istället??) för att rita conturer för e_mt vs e_tt. -0.027<e_mt<0.022, samt -0.063<e_tt<0.064
Arman och Smirnov 2013 använde IC-79 och DC för att constraina e' och e_mt. -6.1e-3 < e_mt < 5.6e-3, -3.6e-2<e'<3.1e-2. Intressant artikel som jag kan hämta metodik ifrån.
DC 2017 constrainade enbart −0.0067 < ϵµτ < 0.0081. Dom satte e' till 0 eftersom "they are highly correlated".

Slutsatser:
Ingen verkar ha använt IC-86 och DC för att constraina. Den senaste joint analysen är från 2013 med IC79.
Det jag vill göra är en blandning av Demidov 2020 och Salvado/DC 2017. 
Demidovs DC-data från 2019 är på samma format som min IC-data. Alltså kanske jag vill använda den istället för min nuvarande DC-metod.
Första steget kanske är att enbart fokusera på IC constraint, för att sen se hur DC blir. 

1 Apr 
Eventsen med e_mm=0, e_mt!= 0 råkade bli med enbart 10 E bins. Blir lite segt att jämföra med emm...


5 Apr
Lägg 3+1 åt sidan helt for now.
Renormalize DC 2015, and check how close that comes to DC 2018.
Gör klart DC utan systematic uncertainty. Du borde få bättre bounds på emt utan syst unc. 
1. Scan only emt for DC, IC, (and maybe gen2) (result is deltachi vs emt plot)
2. Put prior on e' and do the same thing
3. Let e' free and do  the same thing.

Every time i see something in the chi2 plots, i should check that with the probability plots. 

6 Apr
Salvado har 20 E bins och 20 z bins. 

DC:
    Jag har MC och events för DC 2018 från fil.
    Jag har MC och events från plot för DC 2015.
    Gör klart DC 2018, dvs subtrahera muon background (?) och ta med efficiency från notebooken.
    MC 2018 i E-led passar asbra.
    MC 2015 i z-led passar asbra
7 Apr
DC2015 har för många punkter för att kunna simuleras. Behöver sampla ner den. 
DC2018 verkar lovande

9 Apr
Include cascades and treat them as separate data points, so chi2 sums over both track and cascades.
Systematic uncertainties has two types:
    1. Theoretical: pull/penalty with sigma 
    2. Experimental/detector: modeled with f in denominator
Salvados limits dont improve because of the increased E bins since P has no rapid oscillations.

When we put the pulls to 0, I should get a different CL. Fix this!
Try poissonian likelihood.

For PINGU: set oscillation parameters to best fit from nu-fit without NSI and generate the 3gen case. This is my "data"

Compare your DC CL with your IC, and with the DC collab.
You can plot Fig3 from the DC2017 paper that shows the "event importance" with (N_SI - N_NSI)/sqrt(N_SI). Compare IC and DC 
to see which of them has the best statistics (i.e. number of events)

We can ignore the muon background for now.

13 Apr 
no_osc från get_all_events stämmer överens med no_osc från multiply_fluxes (dvs "manuell" no_osc).
Dessa stämmer dock inte exakt med no_osc från DCs artikel. Avviker med 4 till -2 %.
Utan normering kommer avviker min H0 med deras no_osc med som värst -10%. Verkar som normering mot no_osc gör saker värre. Jag vil nog normera mot H0 istället! Det är också logiskt.
Frågan nu är bara ifall jag ska normera inkl eller exkl background. En annan fråga är ifall jag ska normera öht. En normering begränsar mig till E-planet.

Nu stämmer min noosc perfekt med deras (om jag multiplicerar min med 0.79...).
Min H0 är fortfarande sämre, här är faktorn 0.85, och felet mellan 5 och -10%
Det är ok att multiplicera eventsen med ca 0.85! DC har en neutrino event rate best fit på 85%.

H0 är för hög i dom 3 sista Ebinsen, men har rätt form. z>0 bidrar mycket till dessa events. Vill jag göra om min interpolator? (iaf för DC)
Bidraget från m fortsätter upp för höga E, när det egentligen ska minska. 

14 Apr 
Borde jag inte ha separata normeringsparametrar för tracks och cascades? Min deltachi blir ganska mycket sämre då, vilket jag tycker är konstigt.
Just nu optimeras ju en normeringsparameter för båda pidsen.
Att normera mot H0/noosc ger annorlunda kurva, men sämre. Normering mot H0 ger bäst resultat, men fortfarande sämre än utan normering

15 Apr 
IC behöver en prior på e' för att kunna se något där. Kommer inte längre på IC utan prior på emm.
Har gjort emt, emm individuellt för PINGU, och en plot med båda.
Väntar på sims för DC för att kunna göra en ensam emm och en båda.

26 Apr 
Har kört tre olika PINGU jobs med -dm31N 5 -th23N 5 och en nsi 5. Alla tog 7 dagar i coretid.
IC med samma parametrar tog 2 dagar 15 timmar
DC 

30 Apr
På min dator med 6 CPU och hyperthreading disabled tar en pingu med 1*1*1*3*1 parametrar 10 minuter blankt = 20 min/param/core
En dc med totalg 1*1*1*5*1 tra 2 timmar blankt = 144 min/param/core = 2 hr/param/core
=> pingu på 28 cpu tar 90 minuter för 5*5*5 och 714 m = 12h för 10*10*10
                       446 minuter=7.5 h för 5*5*5*5 och 7142m =120 h = 5 d för 10*10*10*10
dc på 28 cpu tar 9 h för 5*5*5, 44h för 5*5*5*5
                71 h för 10*10*10, 710 h = 30 d för 10*10*10*10~

4 Apr 
Att simulera över osc params kan ta längre tid. 5x5x5 dc på 28 cpu tar 13-15 h.
dc 1x1x19 tar 2 h.
8 Apr 
th23_range blir fel. best fit point har index 3 istället för 2. Ej fel i dm31_range

10 Apr 
Assymetrin i formen och den låga chi2 för ett verkar inte kunna "tas bort". Har tittat på hela arrayn och testat olika H0.
Sandhya får samma form (men inverterad då hon endast tog th_23=41.9 för H0 (cf 49.2)), så den dåliga statistiken 
borde bero på nedskalningen av projektet mellan letter of intent och MC release (kolla upp detta)?.
Skulle jag kunna förklara varför PINGU är sämre än DC för just ett? Kolla på formler och 2-neutino approx. Kolla även på P.