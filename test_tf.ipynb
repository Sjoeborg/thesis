{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '~/NuFSGenMC_nominal.dat'\n",
    "df = pd.read_csv(filename, delimiter=' ', names= ['pdg', 'Ereco', 'zreco', 'Etrue', 'ztrue', 'mcweight', 'flux_pion', 'flux_kaon'], skiprows=12)\n",
    "df_small = df.sample(1000, random_state=0)\n",
    "df_small.to_csv('./df_small.csv/', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gp(amplitude, length_scale, observation_noise_variance):\n",
    "  \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n",
    "\n",
    "  # Create the covariance kernel, which will be shared between the prior (which we\n",
    "  # use for maximum likelihood training) and the posterior (which we use for\n",
    "  # posterior predictive sampling)\n",
    "  kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\n",
    "\n",
    "  # Create the GP prior distribution, which we will use to train the model\n",
    "  # parameters.\n",
    "  return tfd.GaussianProcess(\n",
    "      kernel=kernel,\n",
    "      index_points=X,\n",
    "      observation_noise_variance=observation_noise_variance)\n",
    "\n",
    "gp_joint_model = tfd.JointDistributionNamed({\n",
    "    'amplitude': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n",
    "    'length_scale': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n",
    "    'observation_noise_variance': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n",
    "    'observations': build_gp,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gp_joint_model.sample()\n",
    "lp = gp_joint_model.log_prob(x)\n",
    "\n",
    "print(\"sampled {}\".format(x))\n",
    "print(\"log_prob of sample: {}\".format(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Ereco = np.round(df.Ereco,0)\n",
    "df = df.groupby('Ereco').median().reset_index()\n",
    "df['Ebin'] = pd.cut(df.Ereco, bins=500*10**np.linspace(0.0,1.3,14))\n",
    "if len(df) > 5000:\n",
    "    df_subsetted = df.sample(5000, random_state=0)\n",
    "X = np.array(np.log(df_subsetted.Ereco)).reshape(-1,1)\n",
    "y = np.log(df_subsetted.Etrue)\n",
    "kernel2  = 1.0 * RBF() + WhiteKernel(noise_level=3)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel2,random_state=0).fit(X, y)"
   ]
  }
 ]
}